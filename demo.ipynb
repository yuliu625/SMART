{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-09T07:25:17.749662Z",
     "start_time": "2025-05-09T07:25:16.191422Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "由langchain中支持的loader实现加载文档。\n",
    "\"\"\"\n",
    "\n",
    "from langchain_pymupdf4llm import PyMuPDF4LLMLoader\n",
    "from langchain_community.document_loaders.parsers import RapidOCRBlobParser\n",
    "from langchain_community.document_loaders.parsers import LLMImageBlobParser\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class PymupdfTextLoader:\n",
    "    \"\"\"\n",
    "    pymupdf不同加载pdf的方法。\n",
    "    3种方法对应不同精细程度的加载方法。\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        pdf_path: str | Path,\n",
    "    ):\n",
    "        self.pdf_path = Path(pdf_path)\n",
    "        self.loader = None\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        loading_method: str = 'rule',\n",
    "    ) -> list[Document]:\n",
    "        \"\"\"\n",
    "        主要方法。\n",
    "\n",
    "        Args:\n",
    "            loading_method: 加载pdf的方法，指定为['rule', 'ocr', 'vlm', ]中的一个。\n",
    "\n",
    "        Returns:\n",
    "            langchain中的Document对象。\n",
    "            我的默认设置使得加载结果是markdown格式的一个文本对象，会在之后被node-parser处理。\n",
    "        \"\"\"\n",
    "        if loading_method == \"rule\":\n",
    "            self.set_rule_loader()\n",
    "            print(f\"loading text pdf by {loading_method}\")\n",
    "        elif loading_method == \"ocr\":\n",
    "            print(f\"loading text pdf by {loading_method}\")\n",
    "            self.set_ocr_loader()\n",
    "            # loader = PyMuPDF4LLMLoader(\n",
    "            #     file_path=self.pdf_path,\n",
    "            #     mode='single',\n",
    "            #     extract_images=True,\n",
    "            #     images_parser=RapidOCRBlobParser(),\n",
    "            #     table_strategy='lines',\n",
    "            # )\n",
    "            # return loader.load()\n",
    "        elif loading_method == \"vlm\":\n",
    "            print(f\"loading text pdf by {loading_method}\")\n",
    "            self.set_vlm_loader()\n",
    "        documents: list[Document] = self.loader.load()\n",
    "        return documents\n",
    "\n",
    "    def set_rule_loader(self):\n",
    "        \"\"\"\n",
    "        仅提取文档中的文字。\n",
    "        \"\"\"\n",
    "        loader = PyMuPDF4LLMLoader(\n",
    "            file_path=self.pdf_path,\n",
    "            mode='single',\n",
    "            table_strategy='lines',\n",
    "        )\n",
    "        self.loader = loader\n",
    "\n",
    "    def set_ocr_loader(self):\n",
    "        \"\"\"\n",
    "        使用ocr强化文档识别。\n",
    "        \"\"\"\n",
    "        loader = PyMuPDF4LLMLoader(\n",
    "            file_path=self.pdf_path,\n",
    "            mode='single',\n",
    "            extract_images=True,\n",
    "            images_parser=RapidOCRBlobParser(),\n",
    "            table_strategy='lines',\n",
    "        )\n",
    "        self.loader = loader\n",
    "\n",
    "    def set_vlm_loader(self):\n",
    "        \"\"\"\n",
    "        使用VLM强化文档识别。\n",
    "        \"\"\"\n",
    "        loader = PyMuPDF4LLMLoader(\n",
    "            file_path=self.pdf_path,\n",
    "            mode='single',\n",
    "            extract_images=True,\n",
    "            images_parser=LLMImageBlobParser(\n",
    "                model=self._get_vlm()\n",
    "            ),\n",
    "            table_strategy='lines',\n",
    "        )\n",
    "        self.loader = loader\n",
    "\n",
    "    def _get_vlm(self) -> BaseChatModel:\n",
    "        \"\"\"\n",
    "        获取VLM，用于识别pdf文档。\n",
    "        仅在set_vlm_loader中使用。\n",
    "\n",
    "        Returns:\n",
    "            VLM。这里用的是qwen最好的VLM。\n",
    "        \"\"\"\n",
    "        vlm = ChatOpenAI(\n",
    "            model='qwen-vl-max',\n",
    "            base_url=os.environ['DASHSCOPE_API_BASE_URL'],\n",
    "            api_key=os.environ['DASHSCOPE_API_KEY'],\n",
    "        )\n",
    "        return vlm\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T07:25:17.755221Z",
     "start_time": "2025-05-09T07:25:17.751211Z"
    }
   },
   "cell_type": "code",
   "source": "text_loader = PymupdfTextLoader(r\"D:\\dataset\\risk_mas_t\\original_pdf\\1910.13461v1.pdf\")",
   "id": "d85645e3de6df52e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T07:25:17.762765Z",
     "start_time": "2025-05-09T07:25:17.756585Z"
    }
   },
   "cell_type": "code",
   "source": "str(Path(r\"D:\\dataset\\risk_mas_t\\original_pdf\\1910.13461v1.pdf\"))",
   "id": "b68c92e5f559d5c3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\dataset\\\\risk_mas_t\\\\original_pdf\\\\1910.13461v1.pdf'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T07:25:50.727736Z",
     "start_time": "2025-05-09T07:25:41.952008Z"
    }
   },
   "cell_type": "code",
   "source": "text_loader.run(loading_method=\"vlm\")",
   "id": "820d3b35fa1b569e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading text pdf by vlm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-10-31T00:48:45+00:00', 'source': 'D:\\\\dataset\\\\risk_mas_t\\\\original_pdf\\\\1910.13461v1.pdf', 'file_path': 'D:\\\\dataset\\\\risk_mas_t\\\\original_pdf\\\\1910.13461v1.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-10-31T00:48:45+00:00', 'trapped': '', 'modDate': 'D:20191031004845Z', 'creationDate': 'D:20191031004845Z'}, page_content='## **BART: Denoising Sequence-to-Sequence Pre-training for Natural** **Language Generation, Translation, and Comprehension**\\n### **Mike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad,** **Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer** Facebook AI { mikelewis,yinhanliu,naman } @fb.com\\n\\n### **Abstract**\\n\\nWe present BART, a denoising autoencoder\\nfor pretraining sequence-to-sequence models.\\nBART is trained by (1) corrupting text with an\\narbitrary noising function, and (2) learning a\\nmodel to reconstruct the original text. It uses\\na standard Tranformer-based neural machine\\n\\ntranslation architecture which, despite its simplicity, can be seen as generalizing BERT (due\\nto the bidirectional encoder), GPT (with the\\nleft-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel\\nin-filling scheme, where spans of text are replaced with a single mask token. BART is\\nparticularly effective when fine tuned for text\\ngeneration but also works well for comprehension tasks. It matches the performance of\\nRoBERTa with comparable training resources\\non GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\\nBART also provides a 1.1 BLEU increase over\\na back-translation system for machine translation, with only target language pretraining. We\\nalso report ablation experiments that replicate\\nother pretraining schemes within the BART\\nframework, to better measure which factors\\nmost influence end-task performance.\\n### **1 Introduction**\\n\\nSelf-supervised methods have achieved remarkable\\nsuccess in a wide range of NLP tasks (Mikolov et al.,\\n2013; Peters et al., 2018; Devlin et al., 2019; Joshi\\net al., 2019; Yang et al., 2019; Liu et al., 2019).\\nThe most successful approaches have been variants of\\nmasked language models, which are denoising autoencoders that are trained to reconstruct text where a ran\\ndom subset of the words has been masked out. Recent\\n\\nwork has shown gains by improving the distribution of\\nmasked tokens (Joshi et al., 2019), the order in which\\n\\n\\nmasked tokens are predicted (Yang et al., 2019), and the\\navailable context for replacing masked tokens (Dong\\net al., 2019). However, these methods typically focus\\non particular types of end tasks (e.g. span prediction,\\ngeneration, etc.), limiting their applicability.\\nIn this paper, we present BART, which pre-trains\\na model combining Bidirectional and Auto-Regressive\\nTransformers. BART is a denoising autoencoder built\\nwith a sequence-to-sequence model that is applicable\\nto a very wide range of end tasks. Pretraining has\\ntwo stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is\\nlearned to reconstruct the original text. BART uses a\\nstandard Tranformer-based neural machine translation\\n\\narchitecture which, despite its simplicity, can be seen as\\ngeneralizing BERT (due to the bidirectional encoder),\\nGPT (with the left-to-right decoder), and many other\\nmore recent pretraining schemes (see Figure 1).\\nA key advantage of this setup is the noising flexibility; arbitrary transformations can be applied to the original text, including changing its length. We evaluate\\na number of noising approaches, finding the best performance by both randomly shuffling the order of the\\noriginal sentences and using a novel in-filling scheme,\\nwhere arbitrary length spans of text (including zero\\nlength) are replaced with a single mask token. This approach generalizes the original word masking and next\\nsentence prediction objectives in BERT by forcing the\\nmodel to reason more about overall sentence length and\\nmake longer range transformations to the input.\\nBART is particularly effective when fine tuned for\\ntext generation but also works well for comprehension tasks. It matches the performance of RoBERTa\\n(Liu et al., 2019) with comparable training resources\\non GLUE (Wang et al., 2018) and SQuAD (Rajpurkar\\net al., 2016), and achieves new state-of-the-art results\\non a range of abstractive dialogue, question answering, and summarization tasks. For example, it improves performance by 6 ROUGE over previous work\\non XSum (Narayan et al., 2018).\\nBART also opens up new ways of thinking about fine\\ntuning. We present a new scheme for machine translation where a BART model is stacked above a few ad\\nditional transformer layers. These layers are trained\\nto essentially translate the foreign language to noised\\n\\n\\n-----\\n\\n#### B    D  \\n\\n![Bidirectional Encoder represented in a pink oval with arrows pointing in both directions.](#) A _ C _ E\\n\\n(a) BERT: Random tokens are replaced with masks, and\\nthe document is encoded bidirectionally. Missing tokens\\nare predicted independently, so BERT cannot easily be\\nused for generation.\\n\\n\\n|A B C D E Autoregressive Decoder <s> A B C D|A B C D E|\\n|---|---|\\n|<s> A B C D||\\n\\n\\n(b) GPT: Tokens are predicted auto-regressively, meaning\\nGPT can be used for generation. However words can only\\ncondition on leftward context, so it cannot learn bidirec\\n\\n\\n\\n\\n|tional interactions.|nal interactions.|\\n|---|---|\\n|A B C D E Bidirectional Autoregressive Encoder Decoder A _ B _ E <s> A B C D|A B C D E|\\n|A _ B _ E||\\n\\n\\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations. Here, a\\ndocument has been corrupted by replacing spans of text with mask symbols. The corrupted document (left) is encoded with\\na bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\\nFor fine-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the final\\nhidden state of the decoder.\\n\\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\\n\\n\\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\\nThis approach improves performance over a strong\\nback-translation MT baseline by 1.1 BLEU on the\\nWMT Romanian-English benchmark.\\nTo better understand these effects, we also report\\nan ablation analysis that replicates other recently proposed training objectives. This study allows us to carefully control for a number of factors, including data\\nand optimization parameters, which have been shown\\nto be as important for overall performance as the selection of training objectives (Liu et al., 2019). We find\\nthat BART exhibits the most consistently strong performance across the full range of tasks we consider.\\n### **2 Model**\\n\\nBART is a denoising autoencoder that maps a corrupted\\ndocument to the original document it was derived from.\\nIt is implemented as a sequence-to-sequence model\\nwith a bidirectional encoder over corrupted text and a\\nleft-to-right autoregressive decoder. For pre-training,\\nwe optimize the negative log likelihood of the original\\ndocument.\\n\\n**2.1** **Architecture**\\n\\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016)\\nand initialise parameters from *N* (0 *,* 0 *.* 02). For our\\nbase model, we use 6 layers in the encoder and de\\n\\ncoder, and for our large model we use 12 layers in\\neach. The architecture is closely related to that used in\\nBERT, with the following differences: (1) each layer of\\nthe decoder additionally performs cross-attention over\\nthe final hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT\\nuses an additional feed-forward network before word\\nprediction, which BART does not. In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\\n\\n**2.2** **Pre-training BART**\\n\\nBART is trained by corrupting documents and then optimizing a reconstruction loss—the cross-entropy between the decoder’s output and the original document.\\nUnlike existing denoising autoencoders, which are tailored to specific noising schemes, BART allows us to\\napply *any* type of document corruption. In the extreme\\ncase, where all information about the source is lost,\\nBART is equivalent to a language model.\\nWe experiment with several previously proposed and\\nnovel transformations, but we believe there is a significant potential for development of other new alternatives. The transformations we used are summarized\\n\\nbelow, and examples are shown in Figure 2.\\n\\n**Token Masking** Following BERT (Devlin et al.,\\n2019), random tokens are sampled and replaced with\\n\\n[MASK] elements.\\n\\n**Token Deletion** Random tokens are deleted from the\\n\\ninput. In contrast to token masking, the model must\\ndecide which positions are missing inputs.\\n\\n\\n-----\\n\\n#### A _C . _ E . D E . A B C . C . D E . A B Token Masking Sentence Permutation Document Rotation A . C . E . A B C . D E . A _ . D _ E . Token Deletion Text Infilling\\n\\nFigure 2: Transformations for noising the input that we experiment with. These transformations can be composed.\\n\\n\\n**Text Infilling** A number of text spans are sampled,\\nwith span lengths drawn from a Poisson distribution\\n( *λ* = 3). Each span is replaced with a *single* [MASK]\\ntoken. 0-length spans correspond to the insertion of\\n\\n[MASK] tokens. Text infilling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples\\nspan lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of\\n\\n[MASK] tokens of exactly the same length. Text infilling teaches the model to predict how many tokens are\\nmissing from a span.\\n\\n**Sentence Permutation** A document is divided into\\n\\nsentences based on full stops, and these sentences are\\nshuffled in a random order.\\n\\n**Document Rotation** A token is chosen uniformly at\\nrandom, and the document is rotated so that it begins\\nwith that token. This task trains the model to identify\\nthe start of the document.\\n### **3 Fine-tuning BART**\\n\\nThe representations produced by BART can be used in\\nseveral ways for downstream applications.\\n\\n**3.1** **Sequence Classification Tasks**\\n\\nFor sequence classification tasks, the same input is fed\\ninto the encoder and decoder, and the final hidden state\\nof the final decoder token is fed into new multi-class\\nlinear classifier. This approach is related to the CLS\\ntoken in BERT; however we add the additional token\\nto the *end* so that representation for the token in the\\ndecoder can attend to decoder states from the complete\\ninput (Figure 3a).\\n\\n**3.2** **Token Classification Tasks**\\n\\nFor token classification tasks, such as answer endpoint\\nclassification for SQuAD, we feed the complete document into the encoder and decoder, and use the top\\nhidden state of the decoder as a representation for each\\nword. This representation is used to classify the token.\\n\\n**3.3** **Sequence Generation Tasks**\\n\\nBecause BART has an autoregressive decoder, it can be\\ndirectly fine tuned for sequence generation tasks such\\nas abstractive question answering and summarization.\\nIn both of these tasks, information is copied from the\\n\\n\\ninput but manipulated, which is closely related to the\\ndenoising pre-training objective. Here, the encoder input is the input sequence, and the decoder generates\\noutputs autoregressively.\\n\\n**3.4** **Machine Translation**\\n\\nWe also explore using BART to improve machine translation decoders for translating into English. Previous\\nwork Edunov et al. (2019) has shown that models can\\nbe improved by incorporating pre-trained encoders, but\\ngains from using pre-trained language models in decoders have been limited. We show that it is possible\\nto use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that\\nare learned from bitext (see Figure 3b).\\nMore precisely, we replace BART’s encoder embedding layer with a new randomly initialized encoder.\\nThe model is trained end-to-end, which trains the new\\nencoder to map foreign words into an input that BART\\ncan de-noise to English. The new encoder can use a\\nseparate vocabulary from the original BART model.\\nWe train the source encoder in two steps, in both\\ncases backpropagating the cross-entropy loss from the\\noutput of the BART model. In the first step, we freeze\\nmost of BART parameters and only update the randomly initialized source encoder, the BART positional\\nembeddings, and the self-attention input projection matrix of BART’s encoder first layer. In the second step,\\nwe train all model parameters for a small number of\\niterations.\\n### **4 Comparing Pre-training Objectives**\\n\\nBART supports a much wider range of noising schemes\\nduring pre-training than previous work. We compare a\\nrange of options using base-size models (6 encoder and\\n6 decoder layers, with a hidden size of 768), evaluated\\non a representative subset of the tasks we will consider\\nfor the full large scale experiments in *§* 5.\\n\\n**4.1** **Comparison Objectives**\\n\\nWhile many pre-training objectives have been proposed, fair comparisons between these have been difficult to perform, at least in part due to differences in\\ntraining data, training resources, architectural differences between models, and fine-tuning procedures. We\\n\\n\\n-----\\n\\n#### label\\n\\n\\nA B C D E\\n\\n\\n\\n![Pre-trained Encoder diagram with arrows pointing towards it.](#)\\n\\n![A diagram of a pre-trained decoder with an input arrow on the left and multiple output arrows on the right.\\n\\nPre-trained Decoder](#)\\n\\n\\n|Pre-trained Encoder|Col2|Col3|Col4|\\n|---|---|---|---|\\n|||||\\n|||||\\n\\n\\n\\n\\n\\n\\n<s> A B C D\\n\\n#### A B C D E <s> A B C D E\\n\\n(a) To use BART for classification problems, the same\\ninput is fed into the encoder and decoder, and the representation from the final output is used.\\n\\n\\n|Col1|Col2|Col3|Col4|Col5|Col6|\\n|---|---|---|---|---|---|\\n|α β γ δ|||||ε|\\n\\n\\n(b) For machine translation, we learn a small additional\\nencoder that replaces the word embeddings in BART. The\\nnew encoder can use a disjoint vocabulary.\\n\\n\\nFigure 3: Fine tuning BART for classification and translation.\\n\\n\\nre-implement strong pre-training approaches recently\\nproposed for discriminative and generation tasks. We\\naim, as much as possible, to control for differences unrelated to the pre-training objective. However, we do\\nmake minor changes to the learning rate and usage of\\nlayer normalisation in order to improve performance\\n(tuning these separately for each objective). For reference, we compare our implementations with published\\nnumbers from BERT, which was also trained for 1M\\nsteps on a combination of books and Wikipedia data.\\nWe compare the following approaches:\\n\\n**Language Model** Similarly to GPT (Radford et al.,\\n2018), we train a left-to-right Transformer language\\nmodel. This model is equivalent to the BART decoder,\\nwithout cross-attention.\\n\\n**Permuted Language Model** Based on XLNet (Yang\\net al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively. For consistency with other models, we do not implement the\\nrelative positional embeddings or attention across segments from XLNet.\\n\\n**Masked Language Model** Following BERT (Devlin\\net al., 2019), we replace 15% of tokens with [MASK]\\nsymbols, and train the model to independently predict\\nthe original tokens.\\n\\n**Multitask Masked Language Model** As in UniLM\\n(Dong et al., 2019), we train a Masked Language\\nModel with additional self-attention masks. Self at\\ntention masks are chosen randomly in with the follow\\nproportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the first 50% of tokens unmasked\\nand a left-to-right mask for the remainder.\\n\\n**Masked Seq-to-Seq** Inspired by MASS (Song et al.,\\n2019), we mask a span containing 50% of tokens,\\nand train a sequence to sequence model to predict the\\nmasked tokens.\\n\\nFor the Permuted LM, Masked LM and Multitask\\nMasked LM, we use two-stream attention (Yang et al.,\\n2019) to efficiently compute likelihoods of the output\\npart of the sequence (using a diagonal self-attention\\nmask on the output to predict words left-to-right).\\n\\n\\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source\\ninput to the encoder and the target is the decoder output, or (2) adding the source as prefix to the target in\\nthe decoder, with a loss only on the target part of the\\nsequence. We find the former works better for BART\\nmodels, and the latter for other models.\\nTo most directly compare our models on their ability\\nto model their fine-tuning objective (the log likelihood\\nof the human text), we report perplexity in Table 1.\\n\\n**4.2** **Tasks**\\n\\n**SQuAD** (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs. Answers\\nare text spans extracted from a given document context.\\nSimilar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of\\nBART, and additionally pass them to the decoder. The\\nmodel includes classifiers to predict the start and end\\nindices of each token.\\n\\n**MNLI** (Williams et al., 2017), a bitext classification\\ntask to predict whether one sentence entails another.\\nThe fine-tuned model concatenates the two sentences\\nwith appended an EOS token, and passes them to both\\nthe BART encoder and decoder. In contrast to BERT,\\nthe representation of the EOS token is used to classify\\nthe sentences relations.\\n\\n**ELI5** (Fan et al., 2019), a long-form abstractive question answering dataset. Models generate answers conditioned on the concatenation of a question and supporting documents.\\n\\n**XSum** (Narayan et al., 2018), a news summarization\\ndataset with highly abstractive summaries.\\n\\n**ConvAI2** (Dinan et al., 2019), a dialogue response\\ngeneration task, conditioned on context and a persona.\\n\\n**CNN/DM** (Hermann et al., 2015), a news summarization dataset. Summaries here are typically closely\\nrelated to source sentences.\\n\\n**4.3** **Results**\\n\\nResults are shown in Table 1. Several trends are clear:\\n\\n\\n-----\\n\\n**Model** **SQuAD 1.1** **MNLI** **ELI5** **XSum** **ConvAI2** **CNN/DM**\\n\\nF1 Acc PPL PPL PPL PPL\\n\\nBERT Base (Devlin et al., 2019) 88.5 **84.3**  -  -  -  \\nMasked Language Model 90.0 83.5 24.77 7.87 12.59 7.06\\nMasked Seq2seq 87.0 82.1 23.40 6.80 11.43 6.19\\nLanguage Model 76.7 80.1 **21.40** 7.00 11.51 6.56\\nPermuted Language Model 89.1 83.7 24.03 7.69 12.23 6.96\\nMultitask Masked Language Model 89.2 82.4 23.73 7.50 12.39 6.74\\n\\nBART Base\\n\\nw/ Token Masking 90.4 84.1 25.05 7.08 11.73 6.10\\nw/ Token Deletion 90.4 84.1 24.61 6.90 11.46 5.87\\n\\nw/ Text Infilling **90.8** 84.0 24.26 **6.61** **11.05** 5.83\\nw/ Document Rotation 77.2 75.3 53.69 17.14 19.87 10.59\\n\\nw/ Sentence Shuffling 85.4 81.5 41.87 10.93 16.67 7.89\\nw/ Text Infilling + Sentence Shuffling **90.8** 83.8 24.17 6.62 11.12 **5.41**\\n\\nTable 1: Comparison of pre-training objectives. All models are of comparable size and are trained for 1M steps\\non a combination of books and Wikipedia data. Entries in the bottom two blocks are trained on identical data\\nusing the same code-base, and fine-tuned with the same procedures. Entries in the second block are inspired by\\npre-training objectives proposed in previous work, but have been simplified to focus on evaluation objectives (see\\n*§* 4.1). Performance varies considerably across tasks, but the BART models with text infilling demonstrate the most\\nconsistently strong performance.\\n\\n\\n**Performance of pre-training methods varies signifi-**\\n**cantly across tasks** The effectiveness of pre-training\\nmethods is highly dependent on the task. For example, a simple language model achieves the best ELI5\\nperformance, but the worst SQUAD results.\\n\\n**Token masking is crucial** Pre-training objectives\\nbased on rotating documents or permuting sentences\\nperform poorly in isolation. The successful methods\\neither use token deletion or masking, or self-attention\\nmasks. Deletion appears to outperform masking on\\ngeneration tasks.\\n\\n**Left-to-right** **pre-training** **improves** **generation**\\nThe Masked Language Model and the Permuted\\nLanguage Model perform less well than others on\\ngeneration, and are the only models we consider that\\ndo not include left-to-right auto-regressive language\\nmodelling during pre-training.\\n\\n**Bidirectional encoders are crucial for SQuAD** As\\nnoted in previous work (Devlin et al., 2019), just\\nleft-to-right decoder performs poorly on SQuAD, because future context is crucial in classification decisions. However, BART achieves similar performance\\nwith only half the number of bidirectional layers.\\n\\n**The pre-training objective is not the only important**\\n**factor** Our Permuted Language Model performs less\\nwell than XLNet (Yang et al., 2019). Some of this difference is likely due to not including other architectural\\nimprovements, such as relative-position embeddings or\\nsegment-level recurrence.\\n\\n\\n**Pure language models perform best on ELI5** The\\nELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task\\nwhere other models outperform BART. A pure language model performs best, suggesting that BART is\\nless effective when the output is only loosely constrained by the input.\\n\\n**BART achieves the most consistently strong perfor-**\\n**mance.** With the exception of ELI5, BART models\\nusing text-infilling perform well on all tasks.\\n### **5 Large-scale Pre-training Experiments**\\n\\nRecent work has shown that downstream performance\\ncan dramatically improve when pre-training is scaled\\nto large batch sizes (Yang et al., 2019; Liu et al., 2019)\\nand corpora. To test how well BART performs in this\\nregime, and to create a useful model for downstream\\ntasks, we trained BART using the same scale as the\\nRoBERTa model.\\n\\n**5.1** **Experimental Setup**\\n\\nWe pre-train a large model with 12 layers in each of the\\nencoder and decoder, and a hidden size of 1024. Following RoBERTa (Liu et al., 2019), we use a batch size\\nof 8000, and train the model for 500000 steps. Documents are tokenized with the same byte-pair encoding\\nas GPT-2 (Radford et al., 2019). Based on the results in\\nSection *§* 4, we use a combination of text infilling and\\nsentence permutation. We mask 30% of tokens in each\\ndocument, and permute all sentences. Although sentence permutation only shows significant additive gains\\n\\n\\n-----\\n\\n**SQuAD 1.1** **SQuAD 2.0** **MNLI** **SST** **QQP** **QNLI** **STS-B** **RTE** **MRPC** **CoLA**\\n\\nEM/F1 EM/F1 m/mm Acc Acc Acc Acc Acc Acc Mcc\\n\\nBERT 84.1/90.9 79.0/81.8 86.6/- 93.2 91.3 92.3 90.0 70.4 88.0 60.6\\n\\nUniLM -/- 80.5/83.4 87.0/85.9 94.5 - 92.7 - 70.9 - 61.1\\n\\nXLNet **89.0** /94.5 86.1/88.8 89.8/- 95.6 91.8 93.9 91.8 83.8 89.2 63.6\\n\\nRoBERTa 88.9/ **94.6** **86.5/89.4** **90.2/90.2** 96.4 92.2 94.7 **92.4** 86.6 **90.9** **68.0**\\n\\nBART 88.8/ **94.6** 86.1/89.2 89.9/90.1 **96.6** **92.5** **94.9** 91.2 **87.0** 90.4 62.8\\n\\nTable 2: Results for large models on SQuAD and GLUE tasks. BART performs comparably to RoBERTa and\\nXLNet, suggesting that BART’s uni-directional decoder layers do not reduce performance on discriminative tasks.\\n\\n**CNN/DailyMail** **XSum**\\nR1 R2 RL R1 R2 RL\\n\\nLead-3 40.42 17.62 36.67 16.30 1.60 11.95\\n\\nPTGEN (See et al., 2017) 36.44 15.66 33.42 29.70 9.21 23.24\\nPTGEN+COV (See et al., 2017) 39.53 17.28 36.38 28.10 8.02 21.72\\nUniLM 43.33 20.21 40.51    -    -    \\nBERTSUMABS (Liu & Lapata, 2019) 41.72 19.39 38.76 38.76 16.33 31.15\\nBERTSUMEXTABS (Liu & Lapata, 2019) 42.13 19.60 39.18 38.81 16.50 31.27\\n\\nBART **44.16** **21.28** **40.90** **45.14** **22.27** **37.25**\\n\\nTable 3: Results on two standard summarization datasets. BART outperforms previous work on summarization on\\ntwo tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\\n\\n\\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able\\nto learn from this task. To help the model better fit the\\ndata, we disabled dropout for the final 10% of training\\nsteps. We use the same pre-training data as Liu et al.\\n(2019), consisting of 160Gb of news, books, stories,\\nand web text.\\n\\n**5.2** **Discriminative Tasks**\\n\\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and\\nGLUE tasks (Warstadt et al., 2018; Socher et al., 2013;\\nDolan & Brockett, 2005; Agirre et al., 2007; Williams\\net al., 2018; Dagan et al., 2006; Levesque et al., 2011).\\nThe most directly comparable baseline is RoBERTa,\\nwhich was pre-trained with the same resources, but\\na different objective. Overall, BART performs similarly, with only small differences between the models\\non most tasks. suggesting that BART’s improvements\\non generation tasks do not come at the expense of classification performance.\\n\\n**5.3** **Generation Tasks**\\n\\nWe also experiment with several text generation tasks.\\nBART is fine-tuned as a standard sequence-to-sequence\\nmodel from the input to the output text. During finetuning we use a label smoothed cross entropy loss\\n(Pereyra et al., 2017), with the smoothing parameter\\nset to 0.1. During generation, we set beam size as 5,\\nremove duplicated trigrams in beam search, and tuned\\nthe model with min-len, max-len, length penalty on the\\nvalidation set (Fan et al., 2017).\\n\\n\\n**ConvAI2**\\n\\nValid F1 Valid PPL\\n\\nSeq2Seq + Attention 16.02 35.07\\nBest System 19.09 17.51\\nBART **20.72** **11.85**\\n\\nTable 4: BART outperforms previous work on conversational response generation. Perplexities are renormalized based on official tokenizer for ConvAI2.\\n\\n**Summarization** To provide a comparison with the\\nstate-of-the-art in summarization, we present results\\non two summarization datasets, CNN/DailyMail and\\nXSum, which have distinct properties.\\nSummaries in the CNN/DailyMail tend to resemble\\nsource sentences. Extractive models do well here, and\\neven the baseline of the first-three source sentences is\\nhighly competitive. Nevertheless, BART outperforms\\nall existing work.\\nIn contrast, XSum is highly abstractive, and extractive models perform poorly. BART outperforms the\\nbest previous work, which leverages BERT, by roughly\\n6.0 points on all ROUGE metrics—representing a significant advance in performance on this problem. Qualitatively, sample quality is high (see *§* 6).\\n\\n**Dialogue** We evaluate dialogue response generation\\non C ONV AI2 (Dinan et al., 2019), in which agents\\nmust generate responses conditioned on both the previous context and a textually-specified persona. BART\\noutperforms previous work on two automated metrics.\\n\\n\\n-----\\n\\n**ELI5**\\n\\nR1 R2 RL\\n\\nBest Extractive 23.5 3.1 17.5\\n\\nLanguage Model 27.8 4.7 23.1\\nSeq2Seq 28.3 5.1 22.8\\nSeq2Seq Multitask 28.9 5.4 23.1\\nBART **30.6** **6.2** **24.3**\\n\\nTable 5: BART achieves state-of-the-art results on\\n\\nthe challenging ELI5 abstractive question answering\\ndataset. Comparison models are from Fan et al. (2019).\\n\\nRO-EN\\n\\nBaseline 36.80\\n\\nFixed BART 36.29\\n\\nTuned BART **37.96**\\n\\nTable 6: The performance (BLEU) of baseline and\\nBART on WMT’16 RO-EN augmented with backtranslation data. BART improves over a strong backtranslation (BT) baseline by using monolingual English\\npre-training.\\n\\n**Abstractive QA** We use the recently proposed ELI5\\ndataset to test the model’s ability to generate long freeform answers. We find BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains\\na challenging, because answers are only weakly specified by the question.\\n\\n**5.4** **Translation**\\n\\nWe also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data\\nfrom Sennrich et al. (2016). We use a 6-layer\\ntransformer source encoder to map Romanian into\\na representation that BART is able to de-noise into\\nEnglish, following the approach introduced in *§* 3.4.\\nExperiment results are presented in Table 6. We\\ncompare our results against a baseline Transformer\\narchitecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row). We show the\\nperformance of both steps of our model in the fixed\\nBART and tuned BART rows. For each row we\\n\\nexperiment on the original WMT16 Romanian-English\\naugmented with back-translation data. We use a\\nbeam width of 5 and a length penalty of *α* = 1.\\nPreliminary results suggested that our approach was\\nless effective without back-translation data, and prone\\nto overfitting—future work should explore additional\\nregularization techniques.\\n### **6 Qualitative Analysis**\\n\\nBART shows large improvements on summarization\\nmetrics, of up to 6 points over the prior state-of-the-art.\\nTo understand BART’s performance beyond automated\\nmetrics, we analyse its generations qualitatively.\\n\\n\\nTable 7 shows example summaries generated by\\nBART. Examples are taken from WikiNews articles\\npublished after the creation of the pre-training corpus,\\nto eliminate the possibility of the events described being present in the model’s training data. Following\\nNarayan et al. (2018), we remove the first sentence of\\nthe article prior to summarizing it, so there is no easy\\nextractive summary of the document.\\nUnsurprisingly, model output is fluent and grammatical English. However, model output is also highly abstractive, with few phrases copied from the input. The\\noutput is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California). In the first example, inferring that\\nfish are protecting reefs from global warming requires\\nnon-trivial inference from the text. However, the claim\\nthat the work was published in Science is not supported\\nby the source.\\nThese samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\\n### **7 Related Work**\\n\\nEarly methods for pretraining were based on language\\nmodels. GPT (Radford et al., 2018) only models leftward context, which is problematic for some tasks.\\nELMo (Peters et al., 2018) concatenates left-only and\\nright-only representations, but does not pre-train interactions between these features. Radford et al. (2019)\\ndemonstrated that very large language models can act\\nas unsupervised multitask models.\\nBERT (Devlin et al., 2019) introduced masked language modelling, which allows pre-training to learn interactions between left and right context words. Recent work has shown that very strong performance can\\nbe achieved by training for longer (Liu et al., 2019),\\nby tying parameters across layers (Lan et al., 2019),\\nand by masking spans instead of words (Joshi et al.,\\n2019). Predictions are not made auto-regressively, reducing the effectiveness of BERT for generation tasks.\\nUniLM (Dong et al., 2019) fine-tunes BERT with an\\nensemble of masks, some of which allow only leftward\\ncontext. Like BART, this allows UniLM to be used for\\nboth generative and discriminative tasks. A difference\\nis that UniLM predictions are conditionally independent, whereas BART’s are autoregressive. BART reduces the mismatch between pre-training and generation tasks, because the decoder is always trained on uncorrupted context.\\nMASS (Song et al., 2019) is perhaps the most similar\\nmodel to BART. An input sequence where a contiguous\\nspan of tokens is masked is mapped to a sequence consisting of the missing tokens. MASS is less effective\\nfor discriminative tasks, because disjoint sets of tokens\\nare fed into the encoder and decoder.\\n\\nXL-Net (Yang et al., 2019) extends BERT by pre\\n\\n-----\\n\\n**Source Document (abbreviated)** **BART Summary**\\n\\n\\nThe researchers examined three types of coral in reefs off the\\ncoast of Fiji ... The researchers found when fish were plentiful,\\nthey would eat algae and seaweed off the corals, which appeared\\nto leave them more resistant to the bacterium Vibrio coralliilyticus, a bacterium associated with bleaching. The researchers suggested the algae, like warming temperatures, might render the\\ncorals’ chemical defenses less effective, and the fish were protecting the coral by removing the algae.\\n\\nSacoolas, who has immunity as a diplomat’s wife, was involved\\nin a traffic collision ... Prime Minister Johnson was questioned\\nabout the case while speaking to the press at a hospital in Watford. He said, “I hope that Anne Sacoolas will come back ...\\nif we can’t resolve it then of course I will be raising it myself\\npersonally with the White House.”\\n\\nAccording to Syrian state media, government forces began deploying into previously SDF controlled territory yesterday. ...\\nOn October 6, US President Donald Trump and Turkish President Recep Tayyip Erdoan spoke on the phone. Then both nations issued statements speaking of an imminent incursion into\\nnortheast Syria ... . On Wednesday, Turkey began a military\\noffensive with airstrikes followed by a ground invasion.\\n\\nThis is the first time anyone has been recorded to run a full\\nmarathon of 42.195 kilometers (approximately 26 miles) under\\nthis pursued landmark time. It was not, however, an officially\\nsanctioned world record, as it was not an ”open race” of the\\nIAAF. His time was 1 hour 59 minutes 40.2 seconds. Kipchoge\\nran in Vienna, Austria. It was an event specifically designed to\\nhelp Kipchoge break the two hour barrier.\\n\\nPG&E stated it scheduled the blackouts in response to forecasts\\nfor high winds amid dry conditions. The aim is to reduce the risk\\nof wildfires. Nearly 800 thousand customers were scheduled to\\nbe affected by the shutoffs which were expected to last through\\nat least midday tomorrow.\\n\\n\\nFisheries off the coast of Fiji are protecting coral reefs from the effects of global\\nwarming, according to a study in the journal Science.\\n\\nBoris Johnson has said he will raise the is\\nsue of US diplomat Anne Sacoolas’ diplomatic immunity with the White House.\\n\\nSyrian government forces have entered\\nterritory held by the US-backed Syrian\\nDemocratic Forces (SDF) in response to\\nTurkey’s incursion into the region.\\n\\nKenyan runner Eliud Kipchoge has run a\\nmarathon in less than two hours.\\n\\nPower has been turned off to millions of\\n\\ncustomers in California as part of a power\\nshutoff plan.\\n\\n\\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles. For clarity, only relevant\\nexcerpts of the source are shown. Summaries combine information from across the article and prior knowledge.\\n\\n\\ndicting masked tokens auto-regressively in a permuted\\norder. This objective allows predictions to condition on\\nboth left and right context. In contrast, the BART decoder works left-to-right during pre-training, matching\\nthe setting during generation.\\n\\nSeveral papers have explored using pre-trained representations to improve machine translation. The\\nlargest improvements have come from pre-training on\\nboth source and target languages (Song et al., 2019;\\nLample & Conneau, 2019), but this requires pretraining on all languages of interest. Other work has\\nshown that encoders can be improved using pre-trained\\nrepresentations (Edunov et al., 2019), but gains in decoders are more limited. We show how BART can be\\n\\nused to improve machine translation decoders.\\n\\n### **8 Conclusions**\\n\\nWe introduced BART, a pre-training approach that\\nlearns to map corrupted documents to the original.\\nBART achieves similar performance to RoBERTa on\\ndiscriminative tasks, while achieving new state-of-theart results on a number of text generation tasks. Future work should explore new methods for corrupting\\ndocuments for pre-training, perhaps tailoring them to\\nspecific end tasks.\\n\\n\\n-----\\n\\n### **References**\\n\\nEneko Agirre, Llu’is M‘arquez, and Richard Wicentowski (eds.). *Proceedings of the Fourth Interna-*\\n*tional Workshop on Semantic Evaluations (SemEval-*\\n*2007)* . Association for Computational Linguistics,\\nPrague, Czech Republic, June 2007.\\n\\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\\nThe PASCAL recognising textual entailment challenge. In *Machine learning challenges. evaluat-*\\n*ing predictive uncertainty, visual object classifica-*\\n*tion, and recognising tectual entailment*, pp. 177–\\n190. Springer, 2006.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. BERT: Pre-training of deep\\nbidirectional transformers for language understanding. In *Proceedings of the 2019 Conference of the*\\n*North American Chapter of the Association for Com-*\\n*putational Linguistics: Human Language Technolo-*\\n*gies, Volume 1 (Long and Short Papers)*, pp. 4171–\\n4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/\\nv1/N19-1423. [URL https://www.aclweb.](https://www.aclweb.org/anthology/N19-1423)\\n[org/anthology/N19-1423.](https://www.aclweb.org/anthology/N19-1423)\\n\\nEmily Dinan, Varvara Logacheva, Valentin Malykh,\\nAlexander Miller, Kurt Shuster, Jack Urbanek,\\nDouwe Kiela, Arthur Szlam, Iulian Serban, Ryan\\nLowe, et al. The second conversational intelligence challenge (convai2). *arXiv preprint*\\n*arXiv:1902.00098*, 2019.\\n\\nWilliam B Dolan and Chris Brockett. Automatically\\nconstructing a corpus of sentential paraphrases. In\\n*Proceedings of the International Workshop on Para-*\\n*phrasing*, 2005.\\n\\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\\nand Hsiao-Wuen Hon. Unified language model pretraining for natural language understanding and generation. *arXiv preprint arXiv:1905.03197*, 2019.\\n\\nSergey Edunov, Alexei Baevski, and Michael Auli.\\nPre-trained language model representations for language generation. In *Proceedings of the 2019 Con-*\\n*ference of the North American Chapter of the Asso-*\\n*ciation for Computational Linguistics: Human Lan-*\\n*guage Technologies, Volume 1 (Long and Short Pa-*\\n*pers)*, 2019.\\n\\nAngela Fan, David Grangier, and Michael Auli. Controllable abstractive summarization. *arXiv preprint*\\n*arXiv:1711.05217*, 2017.\\n\\nAngela Fan, Yacine Jernite, Ethan Perez, David\\nGrangier, Jason Weston, and Michael Auli. Eli5:\\nLong form question answering. *arXiv preprint*\\n*arXiv:1907.09190*, 2019.\\n\\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). *arXiv preprint arXiv:1606.08415*,\\n2016.\\n\\n\\nKarl Moritz Hermann, Tomas Kocisky, Edward\\nGrefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to\\nread and comprehend. In *Advances in neural infor-*\\n*mation processing systems*, pp. 1693–1701, 2015.\\n\\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\\nLuke Zettlemoyer, and Omer Levy. Spanbert: Improving pre-training by representing and predicting\\nspans. *arXiv preprint arXiv:1907.10529*, 2019.\\n\\nGuillaume Lample and Alexis Conneau. Crosslingual language model pretraining. *arXiv preprint*\\n*arXiv:1901.07291*, 2019.\\n\\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\\nKevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. *arXiv preprint*\\n*arXiv:1909.11942*, 2019.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In *AAAI*\\n*Spring Symposium: Logical Formalizations of Com-*\\n*monsense Reasoning*, volume 46, pp. 47, 2011.\\n\\nYang Liu and Mirella Lapata. Text summarization with pretrained encoders. *arXiv preprint*\\n*arXiv:1908.08345*, 2019.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nLuke Zettlemoyer, and Veselin Stoyanov. Roberta:\\nA robustly optimized bert pretraining approach.\\n*arXiv preprint arXiv:1907.11692*, 2019.\\n\\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\\nDean. Efficient estimation of word representations\\nin vector space. *arXiv preprint arXiv:1301.3781*,\\n2013.\\n\\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\\nDon’t give me the details, just the summary! topicaware convolutional neural networks for extreme\\nsummarization. *arXiv preprint arXiv:1808.08745*,\\n2018.\\n\\nGabriel Pereyra, George Tucker, Jan Chorowski,\\nŁukasz Kaiser, and Geoffrey Hinton. Regularizing\\nneural networks by penalizing confident output distributions. *arXiv preprint arXiv:1701.06548*, 2017.\\n\\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\\nGardner, Christopher Clark, Kenton Lee, and Luke\\nZettlemoyer. Deep contextualized word representations. *arXiv preprint arXiv:1802.05365*, 2018.\\n\\nAlec Radford, Karthik Narasimhan, Tim Salimans,\\nand Ilya Sutskever. Improving language understanding by generative pre-training. *URL*\\n*https://s3-us-west-2.* *amazonaws.* *com/openai-*\\n*assets/researchcovers/languageunsupervised/language*\\n*understanding paper. pdf*, 2018.\\n\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. *OpenAI*\\n*Blog*, 1(8), 2019.\\n\\n\\n-----\\n\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,\\nand Percy Liang. Squad: 100,000+ questions for\\nmachine comprehension of text. *arXiv preprint*\\n*arXiv:1606.05250*, 2016.\\n\\nAbigail See, Peter J Liu, and Christopher D\\nManning. Get to the point: Summarization\\nwith pointer-generator networks. *arXiv preprint*\\n*arXiv:1704.04368*, 2017.\\n\\nRico Sennrich, Barry Haddow, and Alexandra Birch.\\nEdinburgh neural machine translation systems for\\nWMT 16. In *Proceedings of the First Conference*\\n*on Machine Translation: Volume 2, Shared Task Pa-*\\n*pers*, 2016.\\n\\nRichard Socher, Alex Perelygin, Jean Wu, Jason\\nChuang, Christopher D Manning, Andrew Ng, and\\nChristopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.\\nIn *Proceedings of EMNLP*, pp. 1631–1642, 2013.\\n\\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu. Mass: Masked sequence to sequence pretraining for language generation. In *International*\\n*Conference on Machine Learning*, 2019.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. Attention is all you\\nneed. In *Advances in neural information processing*\\n*systems*, pp. 5998–6008, 2017.\\n\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\\nHill, Omer Levy, and Samuel R Bowman. Glue:\\nA multi-task benchmark and analysis platform for\\nnatural language understanding. *arXiv preprint*\\n*arXiv:1804.07461*, 2018.\\n\\nAlex Warstadt, Amanpreet Singh, and Samuel R.\\nBowman. Neural network acceptability judgments.\\n*arXiv preprint 1805.12471*, 2018.\\n\\nAdina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for\\nsentence understanding through inference. *arXiv*\\n*preprint arXiv:1704.05426*, 2017.\\n\\nAdina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In *Proceed-*\\n*ings of NAACL-HLT*, 2018.\\n\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime\\nCarbonell, Ruslan Salakhutdinov, and Quoc V\\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. *arXiv preprint*\\n*arXiv:1906.08237*, 2019.\\n\\n')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T07:25:26.758239Z",
     "start_time": "2025-05-09T07:25:22.495144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loader = PyMuPDF4LLMLoader(\n",
    "    file_path=Path(r\"D:\\dataset\\risk_mas_t\\original_pdf\\1910.13461v1.pdf\"),\n",
    "    mode='single',\n",
    "    # extract_images=True,\n",
    "    # images_parser=LLMImageBlobParser(model=llm),\n",
    "    extract_images=True,\n",
    "    images_parser=RapidOCRBlobParser(),\n",
    "    table_strategy='lines'\n",
    ")\n",
    "loader.load()"
   ],
   "id": "305269f414e87e03",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2019-10-31T00:48:45+00:00', 'source': 'D:\\\\dataset\\\\risk_mas_t\\\\original_pdf\\\\1910.13461v1.pdf', 'file_path': 'D:\\\\dataset\\\\risk_mas_t\\\\original_pdf\\\\1910.13461v1.pdf', 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2019-10-31T00:48:45+00:00', 'trapped': '', 'modDate': 'D:20191031004845Z', 'creationDate': 'D:20191031004845Z'}, page_content='## **BART: Denoising Sequence-to-Sequence Pre-training for Natural** **Language Generation, Translation, and Comprehension**\\n### **Mike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad,** **Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer** Facebook AI { mikelewis,yinhanliu,naman } @fb.com\\n\\n### **Abstract**\\n\\nWe present BART, a denoising autoencoder\\nfor pretraining sequence-to-sequence models.\\nBART is trained by (1) corrupting text with an\\narbitrary noising function, and (2) learning a\\nmodel to reconstruct the original text. It uses\\na standard Tranformer-based neural machine\\n\\ntranslation architecture which, despite its simplicity, can be seen as generalizing BERT (due\\nto the bidirectional encoder), GPT (with the\\nleft-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel\\nin-filling scheme, where spans of text are replaced with a single mask token. BART is\\nparticularly effective when fine tuned for text\\ngeneration but also works well for comprehension tasks. It matches the performance of\\nRoBERTa with comparable training resources\\non GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\\nBART also provides a 1.1 BLEU increase over\\na back-translation system for machine translation, with only target language pretraining. We\\nalso report ablation experiments that replicate\\nother pretraining schemes within the BART\\nframework, to better measure which factors\\nmost influence end-task performance.\\n### **1 Introduction**\\n\\nSelf-supervised methods have achieved remarkable\\nsuccess in a wide range of NLP tasks (Mikolov et al.,\\n2013; Peters et al., 2018; Devlin et al., 2019; Joshi\\net al., 2019; Yang et al., 2019; Liu et al., 2019).\\nThe most successful approaches have been variants of\\nmasked language models, which are denoising autoencoders that are trained to reconstruct text where a ran\\ndom subset of the words has been masked out. Recent\\n\\nwork has shown gains by improving the distribution of\\nmasked tokens (Joshi et al., 2019), the order in which\\n\\n\\nmasked tokens are predicted (Yang et al., 2019), and the\\navailable context for replacing masked tokens (Dong\\net al., 2019). However, these methods typically focus\\non particular types of end tasks (e.g. span prediction,\\ngeneration, etc.), limiting their applicability.\\nIn this paper, we present BART, which pre-trains\\na model combining Bidirectional and Auto-Regressive\\nTransformers. BART is a denoising autoencoder built\\nwith a sequence-to-sequence model that is applicable\\nto a very wide range of end tasks. Pretraining has\\ntwo stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is\\nlearned to reconstruct the original text. BART uses a\\nstandard Tranformer-based neural machine translation\\n\\narchitecture which, despite its simplicity, can be seen as\\ngeneralizing BERT (due to the bidirectional encoder),\\nGPT (with the left-to-right decoder), and many other\\nmore recent pretraining schemes (see Figure 1).\\nA key advantage of this setup is the noising flexibility; arbitrary transformations can be applied to the original text, including changing its length. We evaluate\\na number of noising approaches, finding the best performance by both randomly shuffling the order of the\\noriginal sentences and using a novel in-filling scheme,\\nwhere arbitrary length spans of text (including zero\\nlength) are replaced with a single mask token. This approach generalizes the original word masking and next\\nsentence prediction objectives in BERT by forcing the\\nmodel to reason more about overall sentence length and\\nmake longer range transformations to the input.\\nBART is particularly effective when fine tuned for\\ntext generation but also works well for comprehension tasks. It matches the performance of RoBERTa\\n(Liu et al., 2019) with comparable training resources\\non GLUE (Wang et al., 2018) and SQuAD (Rajpurkar\\net al., 2016), and achieves new state-of-the-art results\\non a range of abstractive dialogue, question answering, and summarization tasks. For example, it improves performance by 6 ROUGE over previous work\\non XSum (Narayan et al., 2018).\\nBART also opens up new ways of thinking about fine\\ntuning. We present a new scheme for machine translation where a BART model is stacked above a few ad\\nditional transformer layers. These layers are trained\\nto essentially translate the foreign language to noised\\n\\n\\n-----\\n\\n#### B    D  \\n\\n![Bidirectional\\nEncoder](#) A _ C _ E\\n\\n(a) BERT: Random tokens are replaced with masks, and\\nthe document is encoded bidirectionally. Missing tokens\\nare predicted independently, so BERT cannot easily be\\nused for generation.\\n\\n\\n|A B C D E Autoregressive Decoder <s> A B C D|A B C D E|\\n|---|---|\\n|<s> A B C D||\\n\\n\\n(b) GPT: Tokens are predicted auto-regressively, meaning\\nGPT can be used for generation. However words can only\\ncondition on leftward context, so it cannot learn bidirec\\n\\n\\n\\n\\n|tional interactions.|nal interactions.|\\n|---|---|\\n|A B C D E Bidirectional Autoregressive Encoder Decoder A _ B _ E <s> A B C D|A B C D E|\\n|A _ B _ E||\\n\\n\\n(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations. Here, a\\ndocument has been corrupted by replacing spans of text with mask symbols. The corrupted document (left) is encoded with\\na bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\\nFor fine-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the final\\nhidden state of the decoder.\\n\\nFigure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\\n\\n\\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model.\\nThis approach improves performance over a strong\\nback-translation MT baseline by 1.1 BLEU on the\\nWMT Romanian-English benchmark.\\nTo better understand these effects, we also report\\nan ablation analysis that replicates other recently proposed training objectives. This study allows us to carefully control for a number of factors, including data\\nand optimization parameters, which have been shown\\nto be as important for overall performance as the selection of training objectives (Liu et al., 2019). We find\\nthat BART exhibits the most consistently strong performance across the full range of tasks we consider.\\n### **2 Model**\\n\\nBART is a denoising autoencoder that maps a corrupted\\ndocument to the original document it was derived from.\\nIt is implemented as a sequence-to-sequence model\\nwith a bidirectional encoder over corrupted text and a\\nleft-to-right autoregressive decoder. For pre-training,\\nwe optimize the negative log likelihood of the original\\ndocument.\\n\\n**2.1** **Architecture**\\n\\nBART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016)\\nand initialise parameters from *N* (0 *,* 0 *.* 02). For our\\nbase model, we use 6 layers in the encoder and de\\n\\ncoder, and for our large model we use 12 layers in\\neach. The architecture is closely related to that used in\\nBERT, with the following differences: (1) each layer of\\nthe decoder additionally performs cross-attention over\\nthe final hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT\\nuses an additional feed-forward network before word\\nprediction, which BART does not. In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\\n\\n**2.2** **Pre-training BART**\\n\\nBART is trained by corrupting documents and then optimizing a reconstruction loss—the cross-entropy between the decoder’s output and the original document.\\nUnlike existing denoising autoencoders, which are tailored to specific noising schemes, BART allows us to\\napply *any* type of document corruption. In the extreme\\ncase, where all information about the source is lost,\\nBART is equivalent to a language model.\\nWe experiment with several previously proposed and\\nnovel transformations, but we believe there is a significant potential for development of other new alternatives. The transformations we used are summarized\\n\\nbelow, and examples are shown in Figure 2.\\n\\n**Token Masking** Following BERT (Devlin et al.,\\n2019), random tokens are sampled and replaced with\\n\\n[MASK] elements.\\n\\n**Token Deletion** Random tokens are deleted from the\\n\\ninput. In contrast to token masking, the model must\\ndecide which positions are missing inputs.\\n\\n\\n-----\\n\\n#### A _C . _ E . D E . A B C . C . D E . A B Token Masking Sentence Permutation Document Rotation A . C . E . A B C . D E . A _ . D _ E . Token Deletion Text Infilling\\n\\nFigure 2: Transformations for noising the input that we experiment with. These transformations can be composed.\\n\\n\\n**Text Infilling** A number of text spans are sampled,\\nwith span lengths drawn from a Poisson distribution\\n( *λ* = 3). Each span is replaced with a *single* [MASK]\\ntoken. 0-length spans correspond to the insertion of\\n\\n[MASK] tokens. Text infilling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples\\nspan lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of\\n\\n[MASK] tokens of exactly the same length. Text infilling teaches the model to predict how many tokens are\\nmissing from a span.\\n\\n**Sentence Permutation** A document is divided into\\n\\nsentences based on full stops, and these sentences are\\nshuffled in a random order.\\n\\n**Document Rotation** A token is chosen uniformly at\\nrandom, and the document is rotated so that it begins\\nwith that token. This task trains the model to identify\\nthe start of the document.\\n### **3 Fine-tuning BART**\\n\\nThe representations produced by BART can be used in\\nseveral ways for downstream applications.\\n\\n**3.1** **Sequence Classification Tasks**\\n\\nFor sequence classification tasks, the same input is fed\\ninto the encoder and decoder, and the final hidden state\\nof the final decoder token is fed into new multi-class\\nlinear classifier. This approach is related to the CLS\\ntoken in BERT; however we add the additional token\\nto the *end* so that representation for the token in the\\ndecoder can attend to decoder states from the complete\\ninput (Figure 3a).\\n\\n**3.2** **Token Classification Tasks**\\n\\nFor token classification tasks, such as answer endpoint\\nclassification for SQuAD, we feed the complete document into the encoder and decoder, and use the top\\nhidden state of the decoder as a representation for each\\nword. This representation is used to classify the token.\\n\\n**3.3** **Sequence Generation Tasks**\\n\\nBecause BART has an autoregressive decoder, it can be\\ndirectly fine tuned for sequence generation tasks such\\nas abstractive question answering and summarization.\\nIn both of these tasks, information is copied from the\\n\\n\\ninput but manipulated, which is closely related to the\\ndenoising pre-training objective. Here, the encoder input is the input sequence, and the decoder generates\\noutputs autoregressively.\\n\\n**3.4** **Machine Translation**\\n\\nWe also explore using BART to improve machine translation decoders for translating into English. Previous\\nwork Edunov et al. (2019) has shown that models can\\nbe improved by incorporating pre-trained encoders, but\\ngains from using pre-trained language models in decoders have been limited. We show that it is possible\\nto use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that\\nare learned from bitext (see Figure 3b).\\nMore precisely, we replace BART’s encoder embedding layer with a new randomly initialized encoder.\\nThe model is trained end-to-end, which trains the new\\nencoder to map foreign words into an input that BART\\ncan de-noise to English. The new encoder can use a\\nseparate vocabulary from the original BART model.\\nWe train the source encoder in two steps, in both\\ncases backpropagating the cross-entropy loss from the\\noutput of the BART model. In the first step, we freeze\\nmost of BART parameters and only update the randomly initialized source encoder, the BART positional\\nembeddings, and the self-attention input projection matrix of BART’s encoder first layer. In the second step,\\nwe train all model parameters for a small number of\\niterations.\\n### **4 Comparing Pre-training Objectives**\\n\\nBART supports a much wider range of noising schemes\\nduring pre-training than previous work. We compare a\\nrange of options using base-size models (6 encoder and\\n6 decoder layers, with a hidden size of 768), evaluated\\non a representative subset of the tasks we will consider\\nfor the full large scale experiments in *§* 5.\\n\\n**4.1** **Comparison Objectives**\\n\\nWhile many pre-training objectives have been proposed, fair comparisons between these have been difficult to perform, at least in part due to differences in\\ntraining data, training resources, architectural differences between models, and fine-tuning procedures. We\\n\\n\\n-----\\n\\n#### label\\n\\n\\nA B C D E\\n\\n\\n\\n![Pre-trained\\nEncoder](#)\\n\\n![Pre-trained\\nDecoder](#)\\n\\n\\n|Pre-trained Encoder|Col2|Col3|Col4|\\n|---|---|---|---|\\n|||||\\n|||||\\n\\n\\n\\n\\n\\n\\n<s> A B C D\\n\\n#### A B C D E <s> A B C D E\\n\\n(a) To use BART for classification problems, the same\\ninput is fed into the encoder and decoder, and the representation from the final output is used.\\n\\n\\n|Col1|Col2|Col3|Col4|Col5|Col6|\\n|---|---|---|---|---|---|\\n|α β γ δ|||||ε|\\n\\n\\n(b) For machine translation, we learn a small additional\\nencoder that replaces the word embeddings in BART. The\\nnew encoder can use a disjoint vocabulary.\\n\\n\\nFigure 3: Fine tuning BART for classification and translation.\\n\\n\\nre-implement strong pre-training approaches recently\\nproposed for discriminative and generation tasks. We\\naim, as much as possible, to control for differences unrelated to the pre-training objective. However, we do\\nmake minor changes to the learning rate and usage of\\nlayer normalisation in order to improve performance\\n(tuning these separately for each objective). For reference, we compare our implementations with published\\nnumbers from BERT, which was also trained for 1M\\nsteps on a combination of books and Wikipedia data.\\nWe compare the following approaches:\\n\\n**Language Model** Similarly to GPT (Radford et al.,\\n2018), we train a left-to-right Transformer language\\nmodel. This model is equivalent to the BART decoder,\\nwithout cross-attention.\\n\\n**Permuted Language Model** Based on XLNet (Yang\\net al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively. For consistency with other models, we do not implement the\\nrelative positional embeddings or attention across segments from XLNet.\\n\\n**Masked Language Model** Following BERT (Devlin\\net al., 2019), we replace 15% of tokens with [MASK]\\nsymbols, and train the model to independently predict\\nthe original tokens.\\n\\n**Multitask Masked Language Model** As in UniLM\\n(Dong et al., 2019), we train a Masked Language\\nModel with additional self-attention masks. Self at\\ntention masks are chosen randomly in with the follow\\nproportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the first 50% of tokens unmasked\\nand a left-to-right mask for the remainder.\\n\\n**Masked Seq-to-Seq** Inspired by MASS (Song et al.,\\n2019), we mask a span containing 50% of tokens,\\nand train a sequence to sequence model to predict the\\nmasked tokens.\\n\\nFor the Permuted LM, Masked LM and Multitask\\nMasked LM, we use two-stream attention (Yang et al.,\\n2019) to efficiently compute likelihoods of the output\\npart of the sequence (using a diagonal self-attention\\nmask on the output to predict words left-to-right).\\n\\n\\nWe experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source\\ninput to the encoder and the target is the decoder output, or (2) adding the source as prefix to the target in\\nthe decoder, with a loss only on the target part of the\\nsequence. We find the former works better for BART\\nmodels, and the latter for other models.\\nTo most directly compare our models on their ability\\nto model their fine-tuning objective (the log likelihood\\nof the human text), we report perplexity in Table 1.\\n\\n**4.2** **Tasks**\\n\\n**SQuAD** (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs. Answers\\nare text spans extracted from a given document context.\\nSimilar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of\\nBART, and additionally pass them to the decoder. The\\nmodel includes classifiers to predict the start and end\\nindices of each token.\\n\\n**MNLI** (Williams et al., 2017), a bitext classification\\ntask to predict whether one sentence entails another.\\nThe fine-tuned model concatenates the two sentences\\nwith appended an EOS token, and passes them to both\\nthe BART encoder and decoder. In contrast to BERT,\\nthe representation of the EOS token is used to classify\\nthe sentences relations.\\n\\n**ELI5** (Fan et al., 2019), a long-form abstractive question answering dataset. Models generate answers conditioned on the concatenation of a question and supporting documents.\\n\\n**XSum** (Narayan et al., 2018), a news summarization\\ndataset with highly abstractive summaries.\\n\\n**ConvAI2** (Dinan et al., 2019), a dialogue response\\ngeneration task, conditioned on context and a persona.\\n\\n**CNN/DM** (Hermann et al., 2015), a news summarization dataset. Summaries here are typically closely\\nrelated to source sentences.\\n\\n**4.3** **Results**\\n\\nResults are shown in Table 1. Several trends are clear:\\n\\n\\n-----\\n\\n**Model** **SQuAD 1.1** **MNLI** **ELI5** **XSum** **ConvAI2** **CNN/DM**\\n\\nF1 Acc PPL PPL PPL PPL\\n\\nBERT Base (Devlin et al., 2019) 88.5 **84.3**  -  -  -  \\nMasked Language Model 90.0 83.5 24.77 7.87 12.59 7.06\\nMasked Seq2seq 87.0 82.1 23.40 6.80 11.43 6.19\\nLanguage Model 76.7 80.1 **21.40** 7.00 11.51 6.56\\nPermuted Language Model 89.1 83.7 24.03 7.69 12.23 6.96\\nMultitask Masked Language Model 89.2 82.4 23.73 7.50 12.39 6.74\\n\\nBART Base\\n\\nw/ Token Masking 90.4 84.1 25.05 7.08 11.73 6.10\\nw/ Token Deletion 90.4 84.1 24.61 6.90 11.46 5.87\\n\\nw/ Text Infilling **90.8** 84.0 24.26 **6.61** **11.05** 5.83\\nw/ Document Rotation 77.2 75.3 53.69 17.14 19.87 10.59\\n\\nw/ Sentence Shuffling 85.4 81.5 41.87 10.93 16.67 7.89\\nw/ Text Infilling + Sentence Shuffling **90.8** 83.8 24.17 6.62 11.12 **5.41**\\n\\nTable 1: Comparison of pre-training objectives. All models are of comparable size and are trained for 1M steps\\non a combination of books and Wikipedia data. Entries in the bottom two blocks are trained on identical data\\nusing the same code-base, and fine-tuned with the same procedures. Entries in the second block are inspired by\\npre-training objectives proposed in previous work, but have been simplified to focus on evaluation objectives (see\\n*§* 4.1). Performance varies considerably across tasks, but the BART models with text infilling demonstrate the most\\nconsistently strong performance.\\n\\n\\n**Performance of pre-training methods varies signifi-**\\n**cantly across tasks** The effectiveness of pre-training\\nmethods is highly dependent on the task. For example, a simple language model achieves the best ELI5\\nperformance, but the worst SQUAD results.\\n\\n**Token masking is crucial** Pre-training objectives\\nbased on rotating documents or permuting sentences\\nperform poorly in isolation. The successful methods\\neither use token deletion or masking, or self-attention\\nmasks. Deletion appears to outperform masking on\\ngeneration tasks.\\n\\n**Left-to-right** **pre-training** **improves** **generation**\\nThe Masked Language Model and the Permuted\\nLanguage Model perform less well than others on\\ngeneration, and are the only models we consider that\\ndo not include left-to-right auto-regressive language\\nmodelling during pre-training.\\n\\n**Bidirectional encoders are crucial for SQuAD** As\\nnoted in previous work (Devlin et al., 2019), just\\nleft-to-right decoder performs poorly on SQuAD, because future context is crucial in classification decisions. However, BART achieves similar performance\\nwith only half the number of bidirectional layers.\\n\\n**The pre-training objective is not the only important**\\n**factor** Our Permuted Language Model performs less\\nwell than XLNet (Yang et al., 2019). Some of this difference is likely due to not including other architectural\\nimprovements, such as relative-position embeddings or\\nsegment-level recurrence.\\n\\n\\n**Pure language models perform best on ELI5** The\\nELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task\\nwhere other models outperform BART. A pure language model performs best, suggesting that BART is\\nless effective when the output is only loosely constrained by the input.\\n\\n**BART achieves the most consistently strong perfor-**\\n**mance.** With the exception of ELI5, BART models\\nusing text-infilling perform well on all tasks.\\n### **5 Large-scale Pre-training Experiments**\\n\\nRecent work has shown that downstream performance\\ncan dramatically improve when pre-training is scaled\\nto large batch sizes (Yang et al., 2019; Liu et al., 2019)\\nand corpora. To test how well BART performs in this\\nregime, and to create a useful model for downstream\\ntasks, we trained BART using the same scale as the\\nRoBERTa model.\\n\\n**5.1** **Experimental Setup**\\n\\nWe pre-train a large model with 12 layers in each of the\\nencoder and decoder, and a hidden size of 1024. Following RoBERTa (Liu et al., 2019), we use a batch size\\nof 8000, and train the model for 500000 steps. Documents are tokenized with the same byte-pair encoding\\nas GPT-2 (Radford et al., 2019). Based on the results in\\nSection *§* 4, we use a combination of text infilling and\\nsentence permutation. We mask 30% of tokens in each\\ndocument, and permute all sentences. Although sentence permutation only shows significant additive gains\\n\\n\\n-----\\n\\n**SQuAD 1.1** **SQuAD 2.0** **MNLI** **SST** **QQP** **QNLI** **STS-B** **RTE** **MRPC** **CoLA**\\n\\nEM/F1 EM/F1 m/mm Acc Acc Acc Acc Acc Acc Mcc\\n\\nBERT 84.1/90.9 79.0/81.8 86.6/- 93.2 91.3 92.3 90.0 70.4 88.0 60.6\\n\\nUniLM -/- 80.5/83.4 87.0/85.9 94.5 - 92.7 - 70.9 - 61.1\\n\\nXLNet **89.0** /94.5 86.1/88.8 89.8/- 95.6 91.8 93.9 91.8 83.8 89.2 63.6\\n\\nRoBERTa 88.9/ **94.6** **86.5/89.4** **90.2/90.2** 96.4 92.2 94.7 **92.4** 86.6 **90.9** **68.0**\\n\\nBART 88.8/ **94.6** 86.1/89.2 89.9/90.1 **96.6** **92.5** **94.9** 91.2 **87.0** 90.4 62.8\\n\\nTable 2: Results for large models on SQuAD and GLUE tasks. BART performs comparably to RoBERTa and\\nXLNet, suggesting that BART’s uni-directional decoder layers do not reduce performance on discriminative tasks.\\n\\n**CNN/DailyMail** **XSum**\\nR1 R2 RL R1 R2 RL\\n\\nLead-3 40.42 17.62 36.67 16.30 1.60 11.95\\n\\nPTGEN (See et al., 2017) 36.44 15.66 33.42 29.70 9.21 23.24\\nPTGEN+COV (See et al., 2017) 39.53 17.28 36.38 28.10 8.02 21.72\\nUniLM 43.33 20.21 40.51    -    -    \\nBERTSUMABS (Liu & Lapata, 2019) 41.72 19.39 38.76 38.76 16.33 31.15\\nBERTSUMEXTABS (Liu & Lapata, 2019) 42.13 19.60 39.18 38.81 16.50 31.27\\n\\nBART **44.16** **21.28** **40.90** **45.14** **22.27** **37.25**\\n\\nTable 3: Results on two standard summarization datasets. BART outperforms previous work on summarization on\\ntwo tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\\n\\n\\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able\\nto learn from this task. To help the model better fit the\\ndata, we disabled dropout for the final 10% of training\\nsteps. We use the same pre-training data as Liu et al.\\n(2019), consisting of 160Gb of news, books, stories,\\nand web text.\\n\\n**5.2** **Discriminative Tasks**\\n\\nTable 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and\\nGLUE tasks (Warstadt et al., 2018; Socher et al., 2013;\\nDolan & Brockett, 2005; Agirre et al., 2007; Williams\\net al., 2018; Dagan et al., 2006; Levesque et al., 2011).\\nThe most directly comparable baseline is RoBERTa,\\nwhich was pre-trained with the same resources, but\\na different objective. Overall, BART performs similarly, with only small differences between the models\\non most tasks. suggesting that BART’s improvements\\non generation tasks do not come at the expense of classification performance.\\n\\n**5.3** **Generation Tasks**\\n\\nWe also experiment with several text generation tasks.\\nBART is fine-tuned as a standard sequence-to-sequence\\nmodel from the input to the output text. During finetuning we use a label smoothed cross entropy loss\\n(Pereyra et al., 2017), with the smoothing parameter\\nset to 0.1. During generation, we set beam size as 5,\\nremove duplicated trigrams in beam search, and tuned\\nthe model with min-len, max-len, length penalty on the\\nvalidation set (Fan et al., 2017).\\n\\n\\n**ConvAI2**\\n\\nValid F1 Valid PPL\\n\\nSeq2Seq + Attention 16.02 35.07\\nBest System 19.09 17.51\\nBART **20.72** **11.85**\\n\\nTable 4: BART outperforms previous work on conversational response generation. Perplexities are renormalized based on official tokenizer for ConvAI2.\\n\\n**Summarization** To provide a comparison with the\\nstate-of-the-art in summarization, we present results\\non two summarization datasets, CNN/DailyMail and\\nXSum, which have distinct properties.\\nSummaries in the CNN/DailyMail tend to resemble\\nsource sentences. Extractive models do well here, and\\neven the baseline of the first-three source sentences is\\nhighly competitive. Nevertheless, BART outperforms\\nall existing work.\\nIn contrast, XSum is highly abstractive, and extractive models perform poorly. BART outperforms the\\nbest previous work, which leverages BERT, by roughly\\n6.0 points on all ROUGE metrics—representing a significant advance in performance on this problem. Qualitatively, sample quality is high (see *§* 6).\\n\\n**Dialogue** We evaluate dialogue response generation\\non C ONV AI2 (Dinan et al., 2019), in which agents\\nmust generate responses conditioned on both the previous context and a textually-specified persona. BART\\noutperforms previous work on two automated metrics.\\n\\n\\n-----\\n\\n**ELI5**\\n\\nR1 R2 RL\\n\\nBest Extractive 23.5 3.1 17.5\\n\\nLanguage Model 27.8 4.7 23.1\\nSeq2Seq 28.3 5.1 22.8\\nSeq2Seq Multitask 28.9 5.4 23.1\\nBART **30.6** **6.2** **24.3**\\n\\nTable 5: BART achieves state-of-the-art results on\\n\\nthe challenging ELI5 abstractive question answering\\ndataset. Comparison models are from Fan et al. (2019).\\n\\nRO-EN\\n\\nBaseline 36.80\\n\\nFixed BART 36.29\\n\\nTuned BART **37.96**\\n\\nTable 6: The performance (BLEU) of baseline and\\nBART on WMT’16 RO-EN augmented with backtranslation data. BART improves over a strong backtranslation (BT) baseline by using monolingual English\\npre-training.\\n\\n**Abstractive QA** We use the recently proposed ELI5\\ndataset to test the model’s ability to generate long freeform answers. We find BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains\\na challenging, because answers are only weakly specified by the question.\\n\\n**5.4** **Translation**\\n\\nWe also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data\\nfrom Sennrich et al. (2016). We use a 6-layer\\ntransformer source encoder to map Romanian into\\na representation that BART is able to de-noise into\\nEnglish, following the approach introduced in *§* 3.4.\\nExperiment results are presented in Table 6. We\\ncompare our results against a baseline Transformer\\narchitecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row). We show the\\nperformance of both steps of our model in the fixed\\nBART and tuned BART rows. For each row we\\n\\nexperiment on the original WMT16 Romanian-English\\naugmented with back-translation data. We use a\\nbeam width of 5 and a length penalty of *α* = 1.\\nPreliminary results suggested that our approach was\\nless effective without back-translation data, and prone\\nto overfitting—future work should explore additional\\nregularization techniques.\\n### **6 Qualitative Analysis**\\n\\nBART shows large improvements on summarization\\nmetrics, of up to 6 points over the prior state-of-the-art.\\nTo understand BART’s performance beyond automated\\nmetrics, we analyse its generations qualitatively.\\n\\n\\nTable 7 shows example summaries generated by\\nBART. Examples are taken from WikiNews articles\\npublished after the creation of the pre-training corpus,\\nto eliminate the possibility of the events described being present in the model’s training data. Following\\nNarayan et al. (2018), we remove the first sentence of\\nthe article prior to summarizing it, so there is no easy\\nextractive summary of the document.\\nUnsurprisingly, model output is fluent and grammatical English. However, model output is also highly abstractive, with few phrases copied from the input. The\\noutput is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California). In the first example, inferring that\\nfish are protecting reefs from global warming requires\\nnon-trivial inference from the text. However, the claim\\nthat the work was published in Science is not supported\\nby the source.\\nThese samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\\n### **7 Related Work**\\n\\nEarly methods for pretraining were based on language\\nmodels. GPT (Radford et al., 2018) only models leftward context, which is problematic for some tasks.\\nELMo (Peters et al., 2018) concatenates left-only and\\nright-only representations, but does not pre-train interactions between these features. Radford et al. (2019)\\ndemonstrated that very large language models can act\\nas unsupervised multitask models.\\nBERT (Devlin et al., 2019) introduced masked language modelling, which allows pre-training to learn interactions between left and right context words. Recent work has shown that very strong performance can\\nbe achieved by training for longer (Liu et al., 2019),\\nby tying parameters across layers (Lan et al., 2019),\\nand by masking spans instead of words (Joshi et al.,\\n2019). Predictions are not made auto-regressively, reducing the effectiveness of BERT for generation tasks.\\nUniLM (Dong et al., 2019) fine-tunes BERT with an\\nensemble of masks, some of which allow only leftward\\ncontext. Like BART, this allows UniLM to be used for\\nboth generative and discriminative tasks. A difference\\nis that UniLM predictions are conditionally independent, whereas BART’s are autoregressive. BART reduces the mismatch between pre-training and generation tasks, because the decoder is always trained on uncorrupted context.\\nMASS (Song et al., 2019) is perhaps the most similar\\nmodel to BART. An input sequence where a contiguous\\nspan of tokens is masked is mapped to a sequence consisting of the missing tokens. MASS is less effective\\nfor discriminative tasks, because disjoint sets of tokens\\nare fed into the encoder and decoder.\\n\\nXL-Net (Yang et al., 2019) extends BERT by pre\\n\\n-----\\n\\n**Source Document (abbreviated)** **BART Summary**\\n\\n\\nThe researchers examined three types of coral in reefs off the\\ncoast of Fiji ... The researchers found when fish were plentiful,\\nthey would eat algae and seaweed off the corals, which appeared\\nto leave them more resistant to the bacterium Vibrio coralliilyticus, a bacterium associated with bleaching. The researchers suggested the algae, like warming temperatures, might render the\\ncorals’ chemical defenses less effective, and the fish were protecting the coral by removing the algae.\\n\\nSacoolas, who has immunity as a diplomat’s wife, was involved\\nin a traffic collision ... Prime Minister Johnson was questioned\\nabout the case while speaking to the press at a hospital in Watford. He said, “I hope that Anne Sacoolas will come back ...\\nif we can’t resolve it then of course I will be raising it myself\\npersonally with the White House.”\\n\\nAccording to Syrian state media, government forces began deploying into previously SDF controlled territory yesterday. ...\\nOn October 6, US President Donald Trump and Turkish President Recep Tayyip Erdoan spoke on the phone. Then both nations issued statements speaking of an imminent incursion into\\nnortheast Syria ... . On Wednesday, Turkey began a military\\noffensive with airstrikes followed by a ground invasion.\\n\\nThis is the first time anyone has been recorded to run a full\\nmarathon of 42.195 kilometers (approximately 26 miles) under\\nthis pursued landmark time. It was not, however, an officially\\nsanctioned world record, as it was not an ”open race” of the\\nIAAF. His time was 1 hour 59 minutes 40.2 seconds. Kipchoge\\nran in Vienna, Austria. It was an event specifically designed to\\nhelp Kipchoge break the two hour barrier.\\n\\nPG&E stated it scheduled the blackouts in response to forecasts\\nfor high winds amid dry conditions. The aim is to reduce the risk\\nof wildfires. Nearly 800 thousand customers were scheduled to\\nbe affected by the shutoffs which were expected to last through\\nat least midday tomorrow.\\n\\n\\nFisheries off the coast of Fiji are protecting coral reefs from the effects of global\\nwarming, according to a study in the journal Science.\\n\\nBoris Johnson has said he will raise the is\\nsue of US diplomat Anne Sacoolas’ diplomatic immunity with the White House.\\n\\nSyrian government forces have entered\\nterritory held by the US-backed Syrian\\nDemocratic Forces (SDF) in response to\\nTurkey’s incursion into the region.\\n\\nKenyan runner Eliud Kipchoge has run a\\nmarathon in less than two hours.\\n\\nPower has been turned off to millions of\\n\\ncustomers in California as part of a power\\nshutoff plan.\\n\\n\\nTable 7: Example summaries from the XSum-tuned BART model on WikiNews articles. For clarity, only relevant\\nexcerpts of the source are shown. Summaries combine information from across the article and prior knowledge.\\n\\n\\ndicting masked tokens auto-regressively in a permuted\\norder. This objective allows predictions to condition on\\nboth left and right context. In contrast, the BART decoder works left-to-right during pre-training, matching\\nthe setting during generation.\\n\\nSeveral papers have explored using pre-trained representations to improve machine translation. The\\nlargest improvements have come from pre-training on\\nboth source and target languages (Song et al., 2019;\\nLample & Conneau, 2019), but this requires pretraining on all languages of interest. Other work has\\nshown that encoders can be improved using pre-trained\\nrepresentations (Edunov et al., 2019), but gains in decoders are more limited. We show how BART can be\\n\\nused to improve machine translation decoders.\\n\\n### **8 Conclusions**\\n\\nWe introduced BART, a pre-training approach that\\nlearns to map corrupted documents to the original.\\nBART achieves similar performance to RoBERTa on\\ndiscriminative tasks, while achieving new state-of-theart results on a number of text generation tasks. Future work should explore new methods for corrupting\\ndocuments for pre-training, perhaps tailoring them to\\nspecific end tasks.\\n\\n\\n-----\\n\\n### **References**\\n\\nEneko Agirre, Llu’is M‘arquez, and Richard Wicentowski (eds.). *Proceedings of the Fourth Interna-*\\n*tional Workshop on Semantic Evaluations (SemEval-*\\n*2007)* . Association for Computational Linguistics,\\nPrague, Czech Republic, June 2007.\\n\\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\\nThe PASCAL recognising textual entailment challenge. In *Machine learning challenges. evaluat-*\\n*ing predictive uncertainty, visual object classifica-*\\n*tion, and recognising tectual entailment*, pp. 177–\\n190. Springer, 2006.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. BERT: Pre-training of deep\\nbidirectional transformers for language understanding. In *Proceedings of the 2019 Conference of the*\\n*North American Chapter of the Association for Com-*\\n*putational Linguistics: Human Language Technolo-*\\n*gies, Volume 1 (Long and Short Papers)*, pp. 4171–\\n4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/\\nv1/N19-1423. [URL https://www.aclweb.](https://www.aclweb.org/anthology/N19-1423)\\n[org/anthology/N19-1423.](https://www.aclweb.org/anthology/N19-1423)\\n\\nEmily Dinan, Varvara Logacheva, Valentin Malykh,\\nAlexander Miller, Kurt Shuster, Jack Urbanek,\\nDouwe Kiela, Arthur Szlam, Iulian Serban, Ryan\\nLowe, et al. The second conversational intelligence challenge (convai2). *arXiv preprint*\\n*arXiv:1902.00098*, 2019.\\n\\nWilliam B Dolan and Chris Brockett. Automatically\\nconstructing a corpus of sentential paraphrases. In\\n*Proceedings of the International Workshop on Para-*\\n*phrasing*, 2005.\\n\\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\\nand Hsiao-Wuen Hon. Unified language model pretraining for natural language understanding and generation. *arXiv preprint arXiv:1905.03197*, 2019.\\n\\nSergey Edunov, Alexei Baevski, and Michael Auli.\\nPre-trained language model representations for language generation. In *Proceedings of the 2019 Con-*\\n*ference of the North American Chapter of the Asso-*\\n*ciation for Computational Linguistics: Human Lan-*\\n*guage Technologies, Volume 1 (Long and Short Pa-*\\n*pers)*, 2019.\\n\\nAngela Fan, David Grangier, and Michael Auli. Controllable abstractive summarization. *arXiv preprint*\\n*arXiv:1711.05217*, 2017.\\n\\nAngela Fan, Yacine Jernite, Ethan Perez, David\\nGrangier, Jason Weston, and Michael Auli. Eli5:\\nLong form question answering. *arXiv preprint*\\n*arXiv:1907.09190*, 2019.\\n\\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). *arXiv preprint arXiv:1606.08415*,\\n2016.\\n\\n\\nKarl Moritz Hermann, Tomas Kocisky, Edward\\nGrefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to\\nread and comprehend. In *Advances in neural infor-*\\n*mation processing systems*, pp. 1693–1701, 2015.\\n\\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\\nLuke Zettlemoyer, and Omer Levy. Spanbert: Improving pre-training by representing and predicting\\nspans. *arXiv preprint arXiv:1907.10529*, 2019.\\n\\nGuillaume Lample and Alexis Conneau. Crosslingual language model pretraining. *arXiv preprint*\\n*arXiv:1901.07291*, 2019.\\n\\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\\nKevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. *arXiv preprint*\\n*arXiv:1909.11942*, 2019.\\n\\nHector J Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In *AAAI*\\n*Spring Symposium: Logical Formalizations of Com-*\\n*monsense Reasoning*, volume 46, pp. 47, 2011.\\n\\nYang Liu and Mirella Lapata. Text summarization with pretrained encoders. *arXiv preprint*\\n*arXiv:1908.08345*, 2019.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nLuke Zettlemoyer, and Veselin Stoyanov. Roberta:\\nA robustly optimized bert pretraining approach.\\n*arXiv preprint arXiv:1907.11692*, 2019.\\n\\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\\nDean. Efficient estimation of word representations\\nin vector space. *arXiv preprint arXiv:1301.3781*,\\n2013.\\n\\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\\nDon’t give me the details, just the summary! topicaware convolutional neural networks for extreme\\nsummarization. *arXiv preprint arXiv:1808.08745*,\\n2018.\\n\\nGabriel Pereyra, George Tucker, Jan Chorowski,\\nŁukasz Kaiser, and Geoffrey Hinton. Regularizing\\nneural networks by penalizing confident output distributions. *arXiv preprint arXiv:1701.06548*, 2017.\\n\\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\\nGardner, Christopher Clark, Kenton Lee, and Luke\\nZettlemoyer. Deep contextualized word representations. *arXiv preprint arXiv:1802.05365*, 2018.\\n\\nAlec Radford, Karthik Narasimhan, Tim Salimans,\\nand Ilya Sutskever. Improving language understanding by generative pre-training. *URL*\\n*https://s3-us-west-2.* *amazonaws.* *com/openai-*\\n*assets/researchcovers/languageunsupervised/language*\\n*understanding paper. pdf*, 2018.\\n\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. *OpenAI*\\n*Blog*, 1(8), 2019.\\n\\n\\n-----\\n\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,\\nand Percy Liang. Squad: 100,000+ questions for\\nmachine comprehension of text. *arXiv preprint*\\n*arXiv:1606.05250*, 2016.\\n\\nAbigail See, Peter J Liu, and Christopher D\\nManning. Get to the point: Summarization\\nwith pointer-generator networks. *arXiv preprint*\\n*arXiv:1704.04368*, 2017.\\n\\nRico Sennrich, Barry Haddow, and Alexandra Birch.\\nEdinburgh neural machine translation systems for\\nWMT 16. In *Proceedings of the First Conference*\\n*on Machine Translation: Volume 2, Shared Task Pa-*\\n*pers*, 2016.\\n\\nRichard Socher, Alex Perelygin, Jean Wu, Jason\\nChuang, Christopher D Manning, Andrew Ng, and\\nChristopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.\\nIn *Proceedings of EMNLP*, pp. 1631–1642, 2013.\\n\\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu. Mass: Masked sequence to sequence pretraining for language generation. In *International*\\n*Conference on Machine Learning*, 2019.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. Attention is all you\\nneed. In *Advances in neural information processing*\\n*systems*, pp. 5998–6008, 2017.\\n\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\\nHill, Omer Levy, and Samuel R Bowman. Glue:\\nA multi-task benchmark and analysis platform for\\nnatural language understanding. *arXiv preprint*\\n*arXiv:1804.07461*, 2018.\\n\\nAlex Warstadt, Amanpreet Singh, and Samuel R.\\nBowman. Neural network acceptability judgments.\\n*arXiv preprint 1805.12471*, 2018.\\n\\nAdina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for\\nsentence understanding through inference. *arXiv*\\n*preprint arXiv:1704.05426*, 2017.\\n\\nAdina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In *Proceed-*\\n*ings of NAACL-HLT*, 2018.\\n\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime\\nCarbonell, Ruslan Salakhutdinov, and Quoc V\\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. *arXiv preprint*\\n*arXiv:1906.08237*, 2019.\\n\\n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
